{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load libraries\n",
    "import pandas as pd\n",
    "from pandas.plotting import scatter_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import model_selection\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num</th>\n",
       "      <th>Date</th>\n",
       "      <th>AveragePrice</th>\n",
       "      <th>Total Volume</th>\n",
       "      <th>4046</th>\n",
       "      <th>4225</th>\n",
       "      <th>4770</th>\n",
       "      <th>Total Bags</th>\n",
       "      <th>Small Bags</th>\n",
       "      <th>Large Bags</th>\n",
       "      <th>XLarge Bags</th>\n",
       "      <th>type</th>\n",
       "      <th>year</th>\n",
       "      <th>region</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2015-12-27</td>\n",
       "      <td>1.33</td>\n",
       "      <td>64236.62</td>\n",
       "      <td>1036.74</td>\n",
       "      <td>54454.85</td>\n",
       "      <td>48.16</td>\n",
       "      <td>8696.87</td>\n",
       "      <td>8603.62</td>\n",
       "      <td>93.25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>conventional</td>\n",
       "      <td>2015</td>\n",
       "      <td>Albany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2015-12-20</td>\n",
       "      <td>1.35</td>\n",
       "      <td>54876.98</td>\n",
       "      <td>674.28</td>\n",
       "      <td>44638.81</td>\n",
       "      <td>58.33</td>\n",
       "      <td>9505.56</td>\n",
       "      <td>9408.07</td>\n",
       "      <td>97.49</td>\n",
       "      <td>0.0</td>\n",
       "      <td>conventional</td>\n",
       "      <td>2015</td>\n",
       "      <td>Albany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2015-12-13</td>\n",
       "      <td>0.93</td>\n",
       "      <td>118220.22</td>\n",
       "      <td>794.70</td>\n",
       "      <td>109149.67</td>\n",
       "      <td>130.50</td>\n",
       "      <td>8145.35</td>\n",
       "      <td>8042.21</td>\n",
       "      <td>103.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>conventional</td>\n",
       "      <td>2015</td>\n",
       "      <td>Albany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2015-12-06</td>\n",
       "      <td>1.08</td>\n",
       "      <td>78992.15</td>\n",
       "      <td>1132.00</td>\n",
       "      <td>71976.41</td>\n",
       "      <td>72.58</td>\n",
       "      <td>5811.16</td>\n",
       "      <td>5677.40</td>\n",
       "      <td>133.76</td>\n",
       "      <td>0.0</td>\n",
       "      <td>conventional</td>\n",
       "      <td>2015</td>\n",
       "      <td>Albany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2015-11-29</td>\n",
       "      <td>1.28</td>\n",
       "      <td>51039.60</td>\n",
       "      <td>941.48</td>\n",
       "      <td>43838.39</td>\n",
       "      <td>75.78</td>\n",
       "      <td>6183.95</td>\n",
       "      <td>5986.26</td>\n",
       "      <td>197.69</td>\n",
       "      <td>0.0</td>\n",
       "      <td>conventional</td>\n",
       "      <td>2015</td>\n",
       "      <td>Albany</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   num        Date  AveragePrice  Total Volume     4046       4225    4770  \\\n",
       "0    0  2015-12-27          1.33      64236.62  1036.74   54454.85   48.16   \n",
       "1    1  2015-12-20          1.35      54876.98   674.28   44638.81   58.33   \n",
       "2    2  2015-12-13          0.93     118220.22   794.70  109149.67  130.50   \n",
       "3    3  2015-12-06          1.08      78992.15  1132.00   71976.41   72.58   \n",
       "4    4  2015-11-29          1.28      51039.60   941.48   43838.39   75.78   \n",
       "\n",
       "   Total Bags  Small Bags  Large Bags  XLarge Bags          type  year  region  \n",
       "0     8696.87     8603.62       93.25          0.0  conventional  2015  Albany  \n",
       "1     9505.56     9408.07       97.49          0.0  conventional  2015  Albany  \n",
       "2     8145.35     8042.21      103.14          0.0  conventional  2015  Albany  \n",
       "3     5811.16     5677.40      133.76          0.0  conventional  2015  Albany  \n",
       "4     6183.95     5986.26      197.69          0.0  conventional  2015  Albany  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv('avocado.csv')\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type\n",
      "conventional    9126\n",
      "organic         9123\n",
      "dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AveragePrice</th>\n",
       "      <th>Total Volume</th>\n",
       "      <th>4046</th>\n",
       "      <th>4225</th>\n",
       "      <th>4770</th>\n",
       "      <th>Total Bags</th>\n",
       "      <th>Small Bags</th>\n",
       "      <th>Large Bags</th>\n",
       "      <th>XLarge Bags</th>\n",
       "      <th>type</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.33</td>\n",
       "      <td>64236.62</td>\n",
       "      <td>1036.74</td>\n",
       "      <td>54454.85</td>\n",
       "      <td>48.16</td>\n",
       "      <td>8696.87</td>\n",
       "      <td>8603.62</td>\n",
       "      <td>93.25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>conventional</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.35</td>\n",
       "      <td>54876.98</td>\n",
       "      <td>674.28</td>\n",
       "      <td>44638.81</td>\n",
       "      <td>58.33</td>\n",
       "      <td>9505.56</td>\n",
       "      <td>9408.07</td>\n",
       "      <td>97.49</td>\n",
       "      <td>0.0</td>\n",
       "      <td>conventional</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.93</td>\n",
       "      <td>118220.22</td>\n",
       "      <td>794.70</td>\n",
       "      <td>109149.67</td>\n",
       "      <td>130.50</td>\n",
       "      <td>8145.35</td>\n",
       "      <td>8042.21</td>\n",
       "      <td>103.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>conventional</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.08</td>\n",
       "      <td>78992.15</td>\n",
       "      <td>1132.00</td>\n",
       "      <td>71976.41</td>\n",
       "      <td>72.58</td>\n",
       "      <td>5811.16</td>\n",
       "      <td>5677.40</td>\n",
       "      <td>133.76</td>\n",
       "      <td>0.0</td>\n",
       "      <td>conventional</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.28</td>\n",
       "      <td>51039.60</td>\n",
       "      <td>941.48</td>\n",
       "      <td>43838.39</td>\n",
       "      <td>75.78</td>\n",
       "      <td>6183.95</td>\n",
       "      <td>5986.26</td>\n",
       "      <td>197.69</td>\n",
       "      <td>0.0</td>\n",
       "      <td>conventional</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   AveragePrice  Total Volume     4046       4225    4770  Total Bags  \\\n",
       "0          1.33      64236.62  1036.74   54454.85   48.16     8696.87   \n",
       "1          1.35      54876.98   674.28   44638.81   58.33     9505.56   \n",
       "2          0.93     118220.22   794.70  109149.67  130.50     8145.35   \n",
       "3          1.08      78992.15  1132.00   71976.41   72.58     5811.16   \n",
       "4          1.28      51039.60   941.48   43838.39   75.78     6183.95   \n",
       "\n",
       "   Small Bags  Large Bags  XLarge Bags          type  year  \n",
       "0     8603.62       93.25          0.0  conventional  2015  \n",
       "1     9408.07       97.49          0.0  conventional  2015  \n",
       "2     8042.21      103.14          0.0  conventional  2015  \n",
       "3     5677.40      133.76          0.0  conventional  2015  \n",
       "4     5986.26      197.69          0.0  conventional  2015  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# class distribution\n",
    "print(dataset.groupby('type').size())\n",
    "X12 = dataset.drop(['Date','region','num'], axis=1)\n",
    "X12.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       AveragePrice  Total Volume       4046       4225     4770  Total Bags  \\\n",
      "0              1.33      64236.62    1036.74   54454.85    48.16     8696.87   \n",
      "1              1.35      54876.98     674.28   44638.81    58.33     9505.56   \n",
      "2              0.93     118220.22     794.70  109149.67   130.50     8145.35   \n",
      "3              1.08      78992.15    1132.00   71976.41    72.58     5811.16   \n",
      "4              1.28      51039.60     941.48   43838.39    75.78     6183.95   \n",
      "5              1.26      55979.78    1184.27   48067.99    43.61     6683.91   \n",
      "6              0.99      83453.76    1368.92   73672.72    93.26     8318.86   \n",
      "7              0.98     109428.33     703.75  101815.36    80.00     6829.22   \n",
      "8              1.02      99811.42    1022.15   87315.57    85.34    11388.36   \n",
      "9              1.07      74338.76     842.40   64757.44   113.00     8625.92   \n",
      "10             1.12      84843.44     924.86   75595.85   117.07     8205.66   \n",
      "11             1.28      64489.17    1582.03   52677.92   105.32    10123.90   \n",
      "12             1.31      61007.10    2268.32   49880.67   101.36     8756.75   \n",
      "13             0.99     106803.39    1204.88   99409.21   154.84     6034.46   \n",
      "14             1.33      69759.01    1028.03   59313.12   150.50     9267.36   \n",
      "15             1.28      76111.27     985.73   65696.86   142.00     9286.68   \n",
      "16             1.11      99172.96     879.45   90062.62   240.79     7990.10   \n",
      "17             1.07     105693.84     689.01   94362.67   335.43    10306.73   \n",
      "18             1.34      79992.09     733.16   67933.79   444.78    10880.36   \n",
      "19             1.33      80043.78     539.65   68666.01   394.90    10443.22   \n",
      "20             1.12     111140.93     584.63  100961.46   368.95     9225.89   \n",
      "21             1.45      75133.10     509.94   62035.06   741.08    11847.02   \n",
      "22             1.11     106757.10     648.75   91949.05   966.61    13192.69   \n",
      "23             1.26      96617.00    1042.10   82049.40  2238.02    11287.48   \n",
      "24             1.05     124055.31     672.25   94693.52  4257.64    24431.90   \n",
      "25             1.35     109252.12     869.45   72600.55  5883.16    29898.96   \n",
      "26             1.37      89534.81     664.23   57545.79  4662.71    26662.08   \n",
      "27             1.27     104849.39     804.01   76688.55  5481.18    21875.65   \n",
      "28             1.32      89631.30     850.58   55400.94  4377.19    29002.59   \n",
      "29             1.07     122743.06     656.71   99220.82    90.32    22775.21   \n",
      "...             ...           ...        ...        ...      ...         ...   \n",
      "18219          1.56    1317000.47   98465.26  270798.27  1839.80   945638.02   \n",
      "18220          1.53    1384683.41  117922.52  287724.61  1703.52   977084.84   \n",
      "18221          1.61    1336979.09  118616.17  280080.34  1270.61   936859.49   \n",
      "18222          1.63    1283987.65  108705.28  259172.13  1490.02   914409.26   \n",
      "18223          1.59    1476651.08  145680.62  323669.83  1580.01  1005593.78   \n",
      "18224          1.51    1517332.70  129541.43  296490.29  1289.07  1089861.24   \n",
      "18225          1.60     271723.08   26996.28   77861.39   117.56   166747.85   \n",
      "18226          1.73     210067.47   33437.98   47165.54   110.40   129353.55   \n",
      "18227          1.63     264691.87   27566.25   60383.57   276.42   176465.63   \n",
      "18228          1.46     347373.17   25990.60   71213.19    79.01   250090.37   \n",
      "18229          1.49     301985.61   34200.18   49139.34    85.58   218560.51   \n",
      "18230          1.64     224798.60   30149.00   38800.64   123.13   155725.83   \n",
      "18231          1.47     275248.53   24732.55   61713.53   243.00   188559.45   \n",
      "18232          1.41     283378.47   22474.66   55360.49   133.41   205409.91   \n",
      "18233          1.80     185974.53   22918.40   33051.14    93.52   129911.47   \n",
      "18234          1.83     189317.99   27049.44   33561.32   439.47   128267.76   \n",
      "18235          1.82     207999.67   33869.12   47435.14   433.52   126261.89   \n",
      "18236          1.48     297190.60   34734.97   62967.74   157.77   199330.12   \n",
      "18237          1.62      15303.40    2325.30    2171.66     0.00    10806.44   \n",
      "18238          1.56      15896.38    2055.35    1499.55     0.00    12341.48   \n",
      "18239          1.56      22128.42    2162.67    3194.25     8.93    16762.57   \n",
      "18240          1.54      17393.30    1832.24    1905.57     0.00    13655.49   \n",
      "18241          1.57      18421.24    1974.26    2482.65     0.00    13964.33   \n",
      "18242          1.56      17597.12    1892.05    1928.36     0.00    13776.71   \n",
      "18243          1.57      15986.17    1924.28    1368.32     0.00    12693.57   \n",
      "18244          1.63      17074.83    2046.96    1529.20     0.00    13498.67   \n",
      "18245          1.71      13888.04    1191.70    3431.50     0.00     9264.84   \n",
      "18246          1.87      13766.76    1191.92    2452.79   727.94     9394.11   \n",
      "18247          1.93      16205.22    1527.63    2981.04   727.01    10969.54   \n",
      "18248          1.62      17489.58    2894.77    2356.13   224.53    12014.15   \n",
      "\n",
      "       Small Bags  Large Bags  XLarge Bags  type  year  \n",
      "0         8603.62       93.25         0.00     1  2015  \n",
      "1         9408.07       97.49         0.00     1  2015  \n",
      "2         8042.21      103.14         0.00     1  2015  \n",
      "3         5677.40      133.76         0.00     1  2015  \n",
      "4         5986.26      197.69         0.00     1  2015  \n",
      "5         6556.47      127.44         0.00     1  2015  \n",
      "6         8196.81      122.05         0.00     1  2015  \n",
      "7         6266.85      562.37         0.00     1  2015  \n",
      "8        11104.53      283.83         0.00     1  2015  \n",
      "9         8061.47      564.45         0.00     1  2015  \n",
      "10        7877.86      327.80         0.00     1  2015  \n",
      "11        9866.27      257.63         0.00     1  2015  \n",
      "12        8379.98      376.77         0.00     1  2015  \n",
      "13        5888.87      145.59         0.00     1  2015  \n",
      "14        8489.10      778.26         0.00     1  2015  \n",
      "15        8665.19      621.49         0.00     1  2015  \n",
      "16        7762.87      227.23         0.00     1  2015  \n",
      "17       10218.93       87.80         0.00     1  2015  \n",
      "18       10745.79      134.57         0.00     1  2015  \n",
      "19       10297.68      145.54         0.00     1  2015  \n",
      "20        9116.34      109.55         0.00     1  2015  \n",
      "21       11768.52       78.50         0.00     1  2015  \n",
      "22       13061.53      131.16         0.00     1  2015  \n",
      "23       11103.49      183.99         0.00     1  2015  \n",
      "24       24290.08      108.49        33.33     1  2015  \n",
      "25       29663.19      235.77         0.00     1  2015  \n",
      "26       26311.76      350.32         0.00     1  2015  \n",
      "27       21662.00      213.65         0.00     1  2015  \n",
      "28       28343.14      659.45         0.00     1  2015  \n",
      "29       22314.99      460.22         0.00     1  2015  \n",
      "...           ...         ...          ...   ...   ...  \n",
      "18219   768242.42   177144.00       251.60     0  2018  \n",
      "18220   774695.74   201878.69       510.41     0  2018  \n",
      "18221   796104.27   140652.84       102.38     0  2018  \n",
      "18222   710654.40   203526.59       228.27     0  2018  \n",
      "18223   858772.69   146808.97        12.12     0  2018  \n",
      "18224   915452.78   174381.57        26.89     0  2018  \n",
      "18225    87108.00    79495.39       144.46     0  2018  \n",
      "18226    73163.12    56020.24       170.19     0  2018  \n",
      "18227   107174.93    69290.70         0.00     0  2018  \n",
      "18228    85835.17   164087.33       167.87     0  2018  \n",
      "18229    99989.62   118314.77       256.12     0  2018  \n",
      "18230   120428.13    35257.73        39.97     0  2018  \n",
      "18231    88497.05    99810.80       251.60     0  2018  \n",
      "18232    70232.59   134666.91       510.41     0  2018  \n",
      "18233    77822.23    51986.86       102.38     0  2018  \n",
      "18234    76091.99    51947.50       228.27     0  2018  \n",
      "18235    89115.78    37133.99        12.12     0  2018  \n",
      "18236   103761.55    95544.39        24.18     0  2018  \n",
      "18237    10569.80      236.64         0.00     0  2018  \n",
      "18238    12114.81      226.67         0.00     0  2018  \n",
      "18239    16510.32      252.25         0.00     0  2018  \n",
      "18240    13401.93      253.56         0.00     0  2018  \n",
      "18241    13698.27      266.06         0.00     0  2018  \n",
      "18242    13553.53      223.18         0.00     0  2018  \n",
      "18243    12437.35      256.22         0.00     0  2018  \n",
      "18244    13066.82      431.85         0.00     0  2018  \n",
      "18245     8940.04      324.80         0.00     0  2018  \n",
      "18246     9351.80       42.31         0.00     0  2018  \n",
      "18247    10919.54       50.00         0.00     0  2018  \n",
      "18248    11988.14       26.01         0.00     0  2018  \n",
      "\n",
      "[18249 rows x 11 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "\n",
    "type = {'conventional': 1,'organic': 0} \n",
    "  \n",
    "X12.type = [type[item] for item in X12.type] \n",
    "print(X12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split-out validation dataset\n",
    "array = X12.values\n",
    "X = array[:,1:14]\n",
    "Y = array[:,10]\n",
    "validation_size = 0.20\n",
    "seed = 7\n",
    "X_train, X_validation, Y_train, Y_validation = model_selection.train_test_split(X, Y, test_size=validation_size, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Test options and evaluation metric\n",
    "seed = 7\n",
    "scoring = 'accuracy'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR: 0.459484 (0.016530)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Bharath\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\Bharath\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\Bharath\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\Bharath\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\Bharath\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\Bharath\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\Bharath\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\Bharath\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\Bharath\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\Bharath\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LDA: 0.375779 (0.011970)\n",
      "KNN: 0.654086 (0.010957)\n",
      "CART: 1.000000 (0.000000)\n",
      "NB: 0.325846 (0.013900)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-38-9213049c9b29>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmodels\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[0mkfold\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_selection\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mKFold\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_splits\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m         \u001b[0mcv_results\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_selection\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcross_val_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkfold\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mscoring\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m         \u001b[0mresults\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcv_results\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m         \u001b[0mnames\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36mcross_val_score\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, error_score)\u001b[0m\n\u001b[0;32m    400\u001b[0m                                 \u001b[0mfit_params\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    401\u001b[0m                                 \u001b[0mpre_dispatch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpre_dispatch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 402\u001b[1;33m                                 error_score=error_score)\n\u001b[0m\u001b[0;32m    403\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mcv_results\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'test_score'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    404\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36mcross_validate\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, error_score)\u001b[0m\n\u001b[0;32m    238\u001b[0m             \u001b[0mreturn_times\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_estimator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreturn_estimator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    239\u001b[0m             error_score=error_score)\n\u001b[1;32m--> 240\u001b[1;33m         for train, test in cv.split(X, y, groups))\n\u001b[0m\u001b[0;32m    241\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    242\u001b[0m     \u001b[0mzipped_scores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mscores\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m    915\u001b[0m             \u001b[1;31m# remaining jobs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    916\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 917\u001b[1;33m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    918\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    919\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    757\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    758\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 759\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    760\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    761\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    714\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    715\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 716\u001b[1;33m             \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    717\u001b[0m             \u001b[1;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    718\u001b[0m             \u001b[1;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    180\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    181\u001b[0m         \u001b[1;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 182\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    183\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    547\u001b[0m         \u001b[1;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    548\u001b[0m         \u001b[1;31m# arguments in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 549\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    550\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    551\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    223\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m             return [func(*args, **kwargs)\n\u001b[1;32m--> 225\u001b[1;33m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[0;32m    226\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    227\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    223\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m             return [func(*args, **kwargs)\n\u001b[1;32m--> 225\u001b[1;33m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[0;32m    226\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    227\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, error_score)\u001b[0m\n\u001b[0;32m    526\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    527\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 528\u001b[1;33m             \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    529\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    530\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    210\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    211\u001b[0m         \u001b[0mseed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrnd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miinfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'i'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 212\u001b[1;33m         \u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msolver_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkernel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_seed\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    213\u001b[0m         \u001b[1;31m# see comment on the other call to np.iinfo in this file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py\u001b[0m in \u001b[0;36m_dense_fit\u001b[1;34m(self, X, y, sample_weight, solver_type, kernel, random_seed)\u001b[0m\n\u001b[0;32m    269\u001b[0m                 \u001b[0mcache_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcache_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcoef0\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcoef0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    270\u001b[0m                 \u001b[0mgamma\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_gamma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 271\u001b[1;33m                 max_iter=self.max_iter, random_seed=random_seed)\n\u001b[0m\u001b[0;32m    272\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    273\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_warn_from_fit_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Spot Check Algorithms\n",
    "# create and configure model\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "model = LogisticRegression(solver='liblinear')\n",
    "# create and configure model\n",
    "model = LogisticRegression(solver='lbfgs')\n",
    "# create and configure model\n",
    "model = LogisticRegression(solver='lbfgs', multi_class='ovr')\n",
    "# create and configure model\n",
    "model = LogisticRegression(solver='lbfgs', multi_class='auto')\n",
    "# create and configure model\n",
    "model = SVC(gamma='auto')\n",
    "# create and configure model\n",
    "model = SVC(gamma='scale')\n",
    "\n",
    "models = []\n",
    "models.append(('LR', LogisticRegression()))\n",
    "models.append(('LDA', LinearDiscriminantAnalysis()))\n",
    "models.append(('KNN', KNeighborsClassifier()))\n",
    "models.append(('CART', DecisionTreeClassifier()))\n",
    "models.append(('NB', GaussianNB()))\n",
    "models.append(('SVM', SVC()))\n",
    "# evaluate each model in turn\n",
    "results = []\n",
    "names = []\n",
    "for name, model in models:\n",
    "\tkfold = model_selection.KFold(n_splits=10, random_state=seed)\n",
    "\tcv_results = model_selection.cross_val_score(model, X_train, Y_train, cv=kfold, scoring=scoring)\n",
    "\tresults.append(cv_results)\n",
    "\tnames.append(name)\n",
    "\tmsg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
    "\tprint(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Compare Algorithms\n",
    "fig = plt.figure()\n",
    "fig.suptitle('Algorithm Comparison')\n",
    "ax = fig.add_subplot(111)\n",
    "plt.boxplot(results)\n",
    "ax.set_xticklabels(names)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6665753424657535\n",
      "[[930 128  60   1]\n",
      " [233 641 213  19]\n",
      " [101 222 785  42]\n",
      " [ 25  55 118  77]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      2015.0       0.72      0.83      0.77      1119\n",
      "      2016.0       0.61      0.58      0.60      1106\n",
      "      2017.0       0.67      0.68      0.67      1150\n",
      "      2018.0       0.55      0.28      0.37       275\n",
      "\n",
      "   micro avg       0.67      0.67      0.67      3650\n",
      "   macro avg       0.64      0.59      0.60      3650\n",
      "weighted avg       0.66      0.67      0.66      3650\n",
      "\n",
      "[2015. 2017. 2015. ... 2018. 2016. 2015.]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Make predictions on validation dataset\n",
    "knn = KNeighborsClassifier()\n",
    "knn.fit(X_train, Y_train)\n",
    "predictions = knn.predict(X_validation)\n",
    "print(accuracy_score(Y_validation, predictions))\n",
    "print(confusion_matrix(Y_validation, predictions))\n",
    "print(classification_report(Y_validation, predictions))\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
